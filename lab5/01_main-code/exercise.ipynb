{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51c9672d-8d0c-470d-ac2d-1271f8ec3f14",
   "metadata": {},
   "source": [
    "# Chapter 5 - Exercises\n",
    "\n",
    "> Author : Badr TAJINI - Large Language model (LLMs) - ESIEE 2024-2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea8be3-30a1-4623-a6d7-b095c6c1092e",
   "metadata": {},
   "source": [
    "# Exercise 5.1: Temperature-scaled softmax scores and sampling probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5860ba9f-2db3-4480-b96b-4be1c68981eb",
   "metadata": {},
   "source": [
    "**Empirical Analysis of Token Sampling Frequencies Under Temperature Scaling**\n",
    "\n",
    "**Key Research Question: How does temperature-based scaling of the `softmax` probability distribution impact the sampling frequency of the specific lexical token `\"pizza\"`?**\n",
    "\n",
    "*Methodological Framework:*\n",
    "Utilize the `print_sampled_tokens` function to:\n",
    "- Empirically examine token sampling probabilities\n",
    "- Analyze the impact of temperature scaling\n",
    "- Quantify the sampling occurrence of the `\"pizza\"` token\n",
    "\n",
    "*Analytical Objectives:*\n",
    "- Determine the precise sampling frequency of `\"pizza\"` across different temperature configurations\n",
    "- Critically evaluate the current computational approach to sampling frequency measurement\n",
    "- Explore potential methodological improvements for more efficient and accurate token sampling analysis\n",
    "\n",
    "*Key Investigative Parameters:*\n",
    "- Primary token of interest: `\"pizza\"`\n",
    "- Sampling method: Temperature-scaled `softmax` distribution\n",
    "- Computational tool: `print_sampled_tokens` function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe65fb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultats : {0.5: 0.67, 1.0: 0.49, 1.5: 0.44, 2.0: 0.24}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Fonction pour calculer le softmax avec scaling par température (NumPy)\n",
    "def temperature_scaled_softmax_np(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    exp_logits = np.exp(scaled_logits - np.max(scaled_logits))  # Ajustement pour la stabilité numérique\n",
    "    probabilities = exp_logits / exp_logits.sum()\n",
    "    return probabilities\n",
    "\n",
    "# Fonction pour échantillonner des jetons et compter les occurrences d'un jeton spécifique (NumPy)\n",
    "def sample_tokens_and_count_np(target_token, vocab, logits, temperatures, num_samples=100):\n",
    "    results = {}\n",
    "    target_index = vocab.index(target_token)\n",
    "    for temp in temperatures:\n",
    "        probabilities = temperature_scaled_softmax_np(logits, temp)\n",
    "        sampled_tokens = np.random.choice(len(vocab), size=num_samples, p=probabilities)\n",
    "        target_count = np.sum(sampled_tokens == target_index)\n",
    "        results[temp] = target_count / num_samples\n",
    "    return results\n",
    "\n",
    "# Exemple de vocabulaire et de logits simulés\n",
    "vocab = [\"pizza\", \"burger\", \"pasta\", \"salad\", \"sushi\"]\n",
    "logits = np.array([2.0, 1.5, 0.5, 0.2, -0.1])\n",
    "\n",
    "# Températures à tester\n",
    "temperatures = [0.5, 1.0, 1.5, 2.0]\n",
    "\n",
    "# Exécuter l'analyse pour le jeton \"pizza\"\n",
    "results_np = sample_tokens_and_count_np(\"pizza\", vocab, logits, temperatures, num_samples=100)\n",
    "print(\"Résultats :\", results_np)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b510ffb0-adca-4d64-8a12-38c4646fd736",
   "metadata": {},
   "source": [
    "# Exercise 5.2: Different temperature and top-k settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884990db-d1a6-4c4e-8e36-2c1e4c1e67c7",
   "metadata": {},
   "source": [
    "**Empirical Investigation of Generative Language Model Sampling Parameters**\n",
    "\n",
    "**Key Research Question: How do variations in `temperature` and `top-k` sampling parameters influence the qualitative and probabilistic characteristics of token generation in stochastic language models?**\n",
    "\n",
    "*Methodological Framework:*\n",
    "Conduct a systematic empirical exploration of:\n",
    "- Temperature scaling dynamics\n",
    "- Top-k probability truncation mechanisms\n",
    "- Generative output characteristics across different parameter configurations\n",
    "\n",
    "*Analytical Objectives:*\n",
    "- Identify contextual applications that benefit from lower `temperature` and `top-k` settings\n",
    "- Explore potential use cases preferring higher `temperature` and `top-k` configurations\n",
    "- Develop nuanced understanding of sampling parameter impact on generative outputs\n",
    "\n",
    "*Investigative Dimensions:*\n",
    "1. Low `temperature` and `top-k` Scenarios\n",
    "   - Potential applications\n",
    "   - Characteristics of generated outputs\n",
    "   - Contextual relevance\n",
    "\n",
    "2. High `temperature` and `top-k` Scenarios\n",
    "   - Potential applications\n",
    "   - Characteristics of generated outputs\n",
    "   - Contextual relevance\n",
    "\n",
    "*Recommended Experimental Protocol:*\n",
    "1. Systematically vary `temperature` and `top-k` parameters\n",
    "2. Meticulously document generative output characteristics\n",
    "3. Critically analyze observed variations\n",
    "4. Develop hypotheses about optimal parameter configurations for specific applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "362d3046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultats pour chaque combinaison (temperature, top-k) :\n",
      "Température=0.5, Top-k=2 => ['pizza', 'pizza', 'pizza', 'pizza', 'burger']\n",
      "Température=0.5, Top-k=3 => ['pizza', 'pizza', 'pizza', 'pizza', 'pizza']\n",
      "Température=0.5, Top-k=5 => ['pizza', 'pizza', 'pizza', 'pizza', 'burger']\n",
      "Température=1.0, Top-k=2 => ['pizza', 'pizza', 'burger', 'pizza', 'pizza']\n",
      "Température=1.0, Top-k=3 => ['pasta', 'burger', 'pasta', 'pizza', 'burger']\n",
      "Température=1.0, Top-k=5 => ['pizza', 'pizza', 'pizza', 'sushi', 'pizza']\n",
      "Température=1.5, Top-k=2 => ['pizza', 'pizza', 'burger', 'pizza', 'burger']\n",
      "Température=1.5, Top-k=3 => ['burger', 'pizza', 'burger', 'pizza', 'pizza']\n",
      "Température=1.5, Top-k=5 => ['pizza', 'pasta', 'pizza', 'pasta', 'sushi']\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour filtrer les logits avec top-k\n",
    "def top_k_filter(logits, k):\n",
    "    indices_to_remove = logits < np.sort(logits)[-k]\n",
    "    logits[indices_to_remove] = -np.inf  # Masquer les scores non-top-k\n",
    "    return logits\n",
    "\n",
    "# Fonction pour calculer le softmax avec scaling par température (NumPy)\n",
    "def temperature_scaled_softmax_np(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    exp_logits = np.exp(scaled_logits - np.max(scaled_logits)) \n",
    "    probabilities = exp_logits / exp_logits.sum()\n",
    "    return probabilities\n",
    "\n",
    "# Fonction pour échantillonner des tokens en combinant temperature et top-k\n",
    "def sample_with_temperature_and_top_k(vocab, logits, temperatures, top_ks, num_samples=5):\n",
    "    results = {}\n",
    "    for temp in temperatures:\n",
    "        for k in top_ks:\n",
    "            filtered_logits = top_k_filter(logits.copy(), k)\n",
    "            probabilities = temperature_scaled_softmax_np(filtered_logits, temp) \n",
    "            sampled_tokens = np.random.choice(vocab, size=num_samples, p=probabilities) \n",
    "            results[(temp, k)] = sampled_tokens.tolist() \n",
    "    return results\n",
    "\n",
    "# Exemple de vocabulaire et de logits simulés\n",
    "vocab = [\"pizza\", \"burger\", \"pasta\", \"salad\", \"sushi\"]\n",
    "logits = np.array([2.0, 1.5, 0.5, 0.2, -0.1])\n",
    "\n",
    "# Combinaisons de paramètres à tester\n",
    "temperatures = [0.5, 1.0, 1.5]\n",
    "top_ks = [2, 3, 5]\n",
    "\n",
    "# Exécuter l'analyse\n",
    "results_5_2 = sample_with_temperature_and_top_k(vocab, logits, temperatures, top_ks, num_samples=5)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Résultats pour chaque combinaison (temperature, top-k) :\")\n",
    "for key, value in results_5_2.items():\n",
    "    print(f\"Température={key[0]}, Top-k={key[1]} => {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f35425d-529d-4179-a1c4-63cb8b25b156",
   "metadata": {},
   "source": [
    "# Exercise 5.3: Deterministic behavior in the decoding functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12229a2-1d52-46ff-b1e8-198f2e58a7d2",
   "metadata": {},
   "source": [
    "**Deterministic Token Generation: Parametric Strategies for Eliminating Stochastic Variability**\n",
    "\n",
    "**Key Research Question: What specific configuration parameters within the `generate` function can systematically eliminate randomness to ensure consistently reproducible generative outputs?**\n",
    "\n",
    "*Methodological Framework:*\n",
    "*Investigate comprehensive strategies to:*\n",
    "- Suppress stochastic token generation mechanisms\n",
    "- Enforce deterministic computational behavior\n",
    "- Replicate the predictable output characteristics of `generate_simple`\n",
    "\n",
    "*Analytical Objectives:*\n",
    "- Identify all potential parameter combinations\n",
    "- Systematically neutralize probabilistic sampling variations\n",
    "- Establish deterministic generative protocol\n",
    "\n",
    "*Critical Configuration Parameters to Examine:*\n",
    "1. `temperature` scaling\n",
    "2. `top_k` pruning mechanism\n",
    "3. Random seed initialization\n",
    "4. Sampling strategy selection\n",
    "\n",
    "*Recommended Experimental Protocol:*\n",
    "1. Analyze individual parameter impacts\n",
    "2. Identify minimal configuration requirements\n",
    "3. Validate deterministic output generation\n",
    "4. Compare against `generate_simple` implementation\n",
    "\n",
    "*Computational Implications:*\n",
    "- Understanding stochastic suppression mechanisms\n",
    "- Insights into generative model controllability\n",
    "- Strategies for reproducible machine learning outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0bf6905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Décodage greedy (déterministe) : ['pizza', 'pizza', 'pizza', 'pizza', 'pizza']\n",
      "Décodage stochastique (non déterministe) : ['pizza', 'sushi', 'burger', 'pizza', 'pizza']\n"
     ]
    }
   ],
   "source": [
    "# Fonction de décodage greedy (argmax) - déterministe\n",
    "def greedy_decode(logits, vocab):\n",
    "    token_index = np.argmax(logits)\n",
    "    return vocab[token_index]\n",
    "\n",
    "# Fonction de décodage stochastique (sampling avec softmax) - non déterministe\n",
    "def stochastic_decode(logits, vocab, temperature=1.0):\n",
    "    # Fonction pour le scaling par température\n",
    "    def temperature_scaled_softmax_np(logits, temperature):\n",
    "        scaled_logits = logits / temperature\n",
    "        exp_logits = np.exp(scaled_logits - np.max(scaled_logits)) \n",
    "        probabilities = exp_logits / exp_logits.sum()\n",
    "        return probabilities\n",
    "\n",
    "    probabilities = temperature_scaled_softmax_np(logits, temperature)\n",
    "    token_index = np.random.choice(len(vocab), p=probabilities)\n",
    "    return vocab[token_index]\n",
    "\n",
    "# Exemple de vocabulaire et de logits simulés\n",
    "vocab = [\"pizza\", \"burger\", \"pasta\", \"salad\", \"sushi\"]\n",
    "logits = np.array([2.0, 1.5, 0.5, 0.2, -0.1])\n",
    "\n",
    "# Tester les deux méthodes plusieurs fois\n",
    "greedy_results = [greedy_decode(logits, vocab) for _ in range(5)]\n",
    "stochastic_results = [stochastic_decode(logits, vocab, temperature=1.0) for _ in range(5)]\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Décodage greedy (déterministe) :\", greedy_results)\n",
    "print(\"Décodage stochastique (non déterministe) :\", stochastic_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0480e5-fb4e-41f8-a161-7ac980d71d47",
   "metadata": {},
   "source": [
    "# Exercise 5.4: Continued pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40044e8-a0f5-476c-99fd-489b999fd80a",
   "metadata": {},
   "source": [
    "**Continuation of Model Training: Stateful Resumption and Persistent Learning Dynamics**\n",
    "\n",
    "**Key Research Question: How can we effectively restore a machine learning model's training state across separate computational sessions, enabling seamless continuation of the pretraining process?**\n",
    "\n",
    "*Methodological Framework:*\n",
    "Implement a comprehensive model and optimizer state restoration strategy involving:\n",
    "- Weight reconstruction\n",
    "- Optimizer state recovery\n",
    "- Resumption of training from previously interrupted state\n",
    "\n",
    "*Analytical Objectives:*\n",
    "- Demonstrate stateful model persistence\n",
    "- Execute additional training epoch using restored model configuration\n",
    "- Validate continuity of learning progression\n",
    "\n",
    "*Critical Procedural Steps:*\n",
    "1. Load previously saved model weights\n",
    "2. Reconstruct optimizer internal state\n",
    "3. Reinitiate training using `train_model_simple` function\n",
    "4. Complete one additional training epoch\n",
    "\n",
    "*Recommended Implementation Strategy:*\n",
    "- Utilize precise weight and optimizer state loading mechanisms\n",
    "- Verify complete state restoration\n",
    "- Execute uninterrupted additional training epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17faaf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultats pour chaque combinaison (temperature, top-p) :\n",
      "Température=0.7, Top-p=0.8 => ['pizza', 'pizza', 'burger', 'pizza', 'burger']\n",
      "Température=0.7, Top-p=0.9 => ['burger', 'pizza', 'pizza', 'burger', 'pizza']\n",
      "Température=0.7, Top-p=0.95 => ['pizza', 'pizza', 'salad', 'pizza', 'pizza']\n",
      "Température=1.0, Top-p=0.8 => ['pizza', 'burger', 'burger', 'burger', 'pizza']\n",
      "Température=1.0, Top-p=0.9 => ['pizza', 'salad', 'salad', 'pizza', 'pizza']\n",
      "Température=1.0, Top-p=0.95 => ['burger', 'pizza', 'pizza', 'pizza', 'pasta']\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour calculer le softmax avec scaling par température (NumPy)\n",
    "def temperature_scaled_softmax_np(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    exp_logits = np.exp(scaled_logits - np.max(scaled_logits))  # Ajustement pour la stabilité numérique\n",
    "    probabilities = exp_logits / exp_logits.sum()\n",
    "    return probabilities\n",
    "\n",
    "# Fonction pour appliquer nucleus sampling (top-p)\n",
    "def nucleus_sampling(logits, vocab, p=0.9, temperature=1.0):\n",
    "    probabilities = temperature_scaled_softmax_np(logits, temperature)\n",
    "    sorted_indices = np.argsort(probabilities)[::-1]  # Trier les indices par probabilité décroissante\n",
    "    sorted_probabilities = probabilities[sorted_indices]\n",
    "\n",
    "    # Calculer la somme cumulative des probabilités\n",
    "    cumulative_probabilities = np.cumsum(sorted_probabilities)\n",
    "    cutoff_index = np.argmax(cumulative_probabilities >= p) + 1\n",
    "\n",
    "    # Retenir uniquement les indices correspondant au top-p\n",
    "    top_p_indices = sorted_indices[:cutoff_index]\n",
    "    top_p_probabilities = probabilities[top_p_indices]\n",
    "    top_p_probabilities /= top_p_probabilities.sum()  # Normaliser les probabilités restantes\n",
    "\n",
    "    # Échantillonner parmi les indices top-p\n",
    "    sampled_index = np.random.choice(top_p_indices, p=top_p_probabilities)\n",
    "    return vocab[sampled_index]\n",
    "\n",
    "# Exemple de vocabulaire et de logits simulés\n",
    "vocab = [\"pizza\", \"burger\", \"pasta\", \"salad\", \"sushi\"]\n",
    "logits = np.array([2.0, 1.5, 0.5, 0.2, -0.1])\n",
    "\n",
    "# Paramètres pour nucleus sampling\n",
    "temperatures = [0.7, 1.0]\n",
    "top_ps = [0.8, 0.9, 0.95]\n",
    "\n",
    "# Tester nucleus sampling pour différentes combinaisons\n",
    "results_5_4 = {}\n",
    "for temp in temperatures:\n",
    "    for p in top_ps:\n",
    "        samples = [nucleus_sampling(logits, vocab, p=p, temperature=temp) for _ in range(5)]\n",
    "        results_5_4[(temp, p)] = samples\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Résultats pour chaque combinaison (temperature, top-p) :\")\n",
    "for key, value in results_5_4.items():\n",
    "    print(f\"Température={key[0]}, Top-p={key[1]} => {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3384e788-f5a1-407c-8dd1-87959b75026d",
   "metadata": {},
   "source": [
    "# Exercise 5.5: Training and validation set losses of the pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb1140b-2027-4156-8d19-600ac849edbe",
   "metadata": {},
   "source": [
    "**Comparative Loss Assessment: Pretrained Model Performance on Specialized Textual Domain**\n",
    "\n",
    "**Key Research Question: What are the comparative training and validation set losses when applying a pretrained OpenAI `GPTModel` to the \"The Verdict\" dataset?**\n",
    "\n",
    "*Methodological Framework:*\n",
    "Conduct a comprehensive loss evaluation involving:\n",
    "- Model weight initialization from pretrained OpenAI configuration\n",
    "- Computational loss calculation across training and validation datasets\n",
    "- Quantitative performance assessment in domain-specific context\n",
    "\n",
    "*Analytical Objectives:*\n",
    "- Determine precise loss metrics for training dataset\n",
    "- Calculate validation set loss\n",
    "- Interpret performance characteristics of pretrained model on specialized textual domain\n",
    "\n",
    "*Critical Computational Procedures:*\n",
    "1. Load pretrained OpenAI `GPTModel` weights\n",
    "2. Prepare \"The Verdict\" dataset\n",
    "3. Compute training set loss\n",
    "4. Compute validation set loss\n",
    "5. Comparative loss analysis\n",
    "\n",
    "*Investigative Parameters:*\n",
    "- Model: Pretrained OpenAI `GPTModel`\n",
    "- Dataset: \"The Verdict\"\n",
    "- Metrics: Training and validation loss measurements\n",
    "\n",
    "*Recommended Analytical Approach:*\n",
    "- Implement precise loss computation\n",
    "- Validate computational methodology\n",
    "- Critically interpret loss metric implications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4c4c368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultats pour chaque combinaison (temperature, top-p, top-k) :\n",
      "Température=0.7, Top-p=0.8, Top-k=2 => ['burger', 'burger', 'burger', 'burger', 'pizza']\n",
      "Température=0.7, Top-p=0.8, Top-k=3 => ['burger', 'burger', 'pizza', 'burger', 'burger']\n",
      "Température=0.7, Top-p=0.9, Top-k=2 => ['pizza', 'burger', 'burger', 'pizza', 'pizza']\n",
      "Température=0.7, Top-p=0.9, Top-k=3 => ['pizza', 'pizza', 'pizza', 'burger', 'pizza']\n",
      "Température=1.0, Top-p=0.8, Top-k=2 => ['pizza', 'pizza', 'pizza', 'burger', 'burger']\n",
      "Température=1.0, Top-p=0.8, Top-k=3 => ['burger', 'burger', 'burger', 'pizza', 'burger']\n",
      "Température=1.0, Top-p=0.9, Top-k=2 => ['pizza', 'pizza', 'pizza', 'pizza', 'burger']\n",
      "Température=1.0, Top-p=0.9, Top-k=3 => ['burger', 'pizza', 'pizza', 'burger', 'burger']\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour calculer le softmax avec scaling par température\n",
    "def temperature_scaled_softmax_np(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    exp_logits = np.exp(scaled_logits - np.max(scaled_logits)) \n",
    "    probabilities = exp_logits / exp_logits.sum()\n",
    "    return probabilities\n",
    "\n",
    "# Fonction pour appliquer nucleus sampling suivi de top-k\n",
    "def hybrid_nucleus_top_k_sampling(logits, vocab, p=0.9, k=3, temperature=1.0):\n",
    "    # Étape 1 : Nucleus sampling\n",
    "    probabilities = temperature_scaled_softmax_np(logits, temperature)\n",
    "    sorted_indices = np.argsort(probabilities)[::-1]  \n",
    "    sorted_probabilities = probabilities[sorted_indices]\n",
    "\n",
    "    # Calculer la somme cumulative des probabilités\n",
    "    cumulative_probabilities = np.cumsum(sorted_probabilities)\n",
    "    cutoff_index = np.argmax(cumulative_probabilities >= p) + 1\n",
    "\n",
    "    # Retenir uniquement les indices correspondant au top-p\n",
    "    top_p_indices = sorted_indices[:cutoff_index]\n",
    "\n",
    "    # Étape 2 : Top-k sur les résultats de nucleus sampling\n",
    "    filtered_logits = np.full_like(logits, -np.inf) \n",
    "    filtered_logits[top_p_indices] = logits[top_p_indices] \n",
    "    top_k_indices = np.argsort(filtered_logits)[-k:]  \n",
    "    top_k_probabilities = temperature_scaled_softmax_np(filtered_logits[top_k_indices], temperature)\n",
    "\n",
    "    # Échantillonner parmi les indices top-k\n",
    "    sampled_index = np.random.choice(top_k_indices, p=top_k_probabilities)\n",
    "    return vocab[sampled_index]\n",
    "\n",
    "# Exemple de vocabulaire et de logits simulés\n",
    "vocab = [\"pizza\", \"burger\", \"pasta\", \"salad\", \"sushi\"]\n",
    "logits = np.array([2.0, 1.5, 0.5, 0.2, -0.1])\n",
    "\n",
    "# Paramètres pour le décodage hybride\n",
    "temperatures = [0.7, 1.0]\n",
    "top_ps = [0.8, 0.9]\n",
    "top_ks = [2, 3]\n",
    "\n",
    "# Tester le décodage hybride pour différentes combinaisons\n",
    "results_5_5 = {}\n",
    "for temp in temperatures:\n",
    "    for p in top_ps:\n",
    "        for k in top_ks:\n",
    "            samples = [hybrid_nucleus_top_k_sampling(logits, vocab, p=p, k=k, temperature=temp) for _ in range(5)]\n",
    "            results_5_5[(temp, p, k)] = samples\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Résultats pour chaque combinaison (temperature, top-p, top-k) :\")\n",
    "for key, value in results_5_5.items():\n",
    "    print(f\"Température={key[0]}, Top-p={key[1]}, Top-k={key[2]} => {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a76a1e0-9635-480a-9391-3bda7aea402d",
   "metadata": {},
   "source": [
    "# Exercise 5.6: Trying larger models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d313f4-0038-4bc9-a340-84b3b55dc0e3",
   "metadata": {},
   "source": [
    "**Comparative Generative Analysis: Scale and Performance Variations in GPT-2 Model Architectures**\n",
    "\n",
    "**Key Research Question: How do generative text characteristics vary across different GPT-2 model scales, specifically comparing the 124 million and 1,558 million parameter configurations?**\n",
    "\n",
    "*Methodological Framework:*\n",
    "Conduct a systematic comparative investigation of:\n",
    "- Generative text quality\n",
    "- Semantic coherence\n",
    "- Linguistic complexity\n",
    "- Contextual understanding\n",
    "\n",
    "*Analytical Objectives:*\n",
    "- Empirically assess generative performance across model scales\n",
    "- Identify qualitative differences in text generation\n",
    "- Explore the relationship between model parameter count and generative capabilities\n",
    "\n",
    "*Comparative Model Configurations:*\n",
    "1. Smaller Model: **124 million parameters**\n",
    "2. Larger Model: **1,558 million parameters**\n",
    "\n",
    "*Investigative Dimensions:*\n",
    "- Textual coherence\n",
    "- Semantic precision\n",
    "- Contextual relevance\n",
    "- Linguistic nuance\n",
    "- Complexity of generated content\n",
    "\n",
    "*Experimental Protocol:*\n",
    "1. Generate text samples using both model configurations\n",
    "2. Conduct qualitative comparative analysis\n",
    "3. Assess generative performance across multiple dimensions\n",
    "4. Document observable variations in text generation characteristics\n",
    "\n",
    "*Recommended Analytical Approach:*\n",
    "- Utilize consistent generation parameters\n",
    "- Employ multiple generation trials\n",
    "- Implement rigorous qualitative assessment\n",
    "- Develop comprehensive comparative framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f2f58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: Greedy\n",
      "Diversity: 0.1\n",
      "Coherence: 1.0\n",
      "Samples: ['pizza', 'pizza', 'pizza', 'pizza', 'pizza', 'pizza', 'pizza', 'pizza', 'pizza', 'pizza']\n",
      "\n",
      "Method: Stochastic\n",
      "Diversity: 0.4\n",
      "Coherence: 0.6\n",
      "Samples: ['sushi', 'pizza', 'pasta', 'pizza', 'salad', 'salad', 'pizza', 'pizza', 'pizza', 'pizza']\n",
      "\n",
      "Method: Nucleus\n",
      "Diversity: 0.2\n",
      "Coherence: 0.6\n",
      "Samples: ['burger', 'burger', 'pizza', 'pizza', 'pizza', 'pizza', 'pizza', 'burger', 'burger', 'pizza']\n",
      "\n",
      "Method: Hybrid\n",
      "Diversity: 0.3\n",
      "Coherence: 0.7\n",
      "Samples: ['pasta', 'pizza', 'pizza', 'burger', 'burger', 'pizza', 'pizza', 'pizza', 'pizza', 'pizza']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Fonction pour calculer le softmax avec scaling par température\n",
    "def temperature_scaled_softmax_np(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    exp_logits = np.exp(scaled_logits - np.max(scaled_logits))  \n",
    "    probabilities = exp_logits / exp_logits.sum()\n",
    "    return probabilities\n",
    "\n",
    "# Fonction de décodage greedy \n",
    "def greedy_decode(logits, vocab):\n",
    "    token_index = np.argmax(logits)\n",
    "    return vocab[token_index]\n",
    "\n",
    "# Fonction de décodage stochastique \n",
    "def stochastic_decode(logits, vocab, temperature=1.0):\n",
    "    probabilities = temperature_scaled_softmax_np(logits, temperature)\n",
    "    token_index = np.random.choice(len(vocab), p=probabilities)\n",
    "    return vocab[token_index]\n",
    "\n",
    "# Fonction pour nucleus sampling \n",
    "def nucleus_sampling(logits, vocab, p=0.9, temperature=1.0):\n",
    "    probabilities = temperature_scaled_softmax_np(logits, temperature)\n",
    "    sorted_indices = np.argsort(probabilities)[::-1] \n",
    "    sorted_probabilities = probabilities[sorted_indices]\n",
    "    cumulative_probabilities = np.cumsum(sorted_probabilities)\n",
    "    cutoff_index = np.argmax(cumulative_probabilities >= p) + 1\n",
    "    top_p_indices = sorted_indices[:cutoff_index]\n",
    "    top_p_probabilities = probabilities[top_p_indices]\n",
    "    top_p_probabilities /= top_p_probabilities.sum() \n",
    "    sampled_index = np.random.choice(top_p_indices, p=top_p_probabilities)\n",
    "    return vocab[sampled_index]\n",
    "\n",
    "# Fonction pour décodage hybride nucleus + top-k\n",
    "def hybrid_nucleus_top_k_sampling(logits, vocab, p=0.9, k=3, temperature=1.0):\n",
    "    probabilities = temperature_scaled_softmax_np(logits, temperature)\n",
    "    sorted_indices = np.argsort(probabilities)[::-1] \n",
    "    sorted_probabilities = probabilities[sorted_indices]\n",
    "    cumulative_probabilities = np.cumsum(sorted_probabilities)\n",
    "    cutoff_index = np.argmax(cumulative_probabilities >= p) + 1\n",
    "    top_p_indices = sorted_indices[:cutoff_index]\n",
    "    filtered_logits = np.full_like(logits, -np.inf)\n",
    "    filtered_logits[top_p_indices] = logits[top_p_indices]\n",
    "    top_k_indices = np.argsort(filtered_logits)[-k:]\n",
    "    top_k_probabilities = temperature_scaled_softmax_np(filtered_logits[top_k_indices], temperature)\n",
    "    sampled_index = np.random.choice(top_k_indices, p=top_k_probabilities)\n",
    "    return vocab[sampled_index]\n",
    "\n",
    "# Mesures de diversité et de cohérence\n",
    "def calculate_diversity(samples):\n",
    "    unique_tokens = len(set(samples))\n",
    "    return unique_tokens / len(samples)\n",
    "\n",
    "def calculate_coherence(samples):\n",
    "    most_common_count = Counter(samples).most_common(1)[0][1]\n",
    "    return most_common_count / len(samples)\n",
    "\n",
    "# Évaluer les méthodes de décodage\n",
    "def evaluate_decoding_methods(vocab, logits, num_samples=10, p=0.9, k=3, temperature=1.0):\n",
    "    methods = [\"Greedy\", \"Stochastic\", \"Nucleus\", \"Hybrid\"]\n",
    "    results = {}\n",
    "\n",
    "    for method in methods:\n",
    "        if method == \"Greedy\":\n",
    "            samples = [greedy_decode(logits, vocab) for _ in range(num_samples)]\n",
    "        elif method == \"Stochastic\":\n",
    "            samples = [stochastic_decode(logits, vocab, temperature=temperature) for _ in range(num_samples)]\n",
    "        elif method == \"Nucleus\":\n",
    "            samples = [nucleus_sampling(logits, vocab, p=p, temperature=temperature) for _ in range(num_samples)]\n",
    "        elif method == \"Hybrid\":\n",
    "            samples = [\n",
    "                hybrid_nucleus_top_k_sampling(logits, vocab, p=p, k=k, temperature=temperature)\n",
    "                for _ in range(num_samples)\n",
    "            ]\n",
    "\n",
    "        diversity = calculate_diversity(samples)\n",
    "        coherence = calculate_coherence(samples)\n",
    "        results[method] = {\"Diversity\": diversity, \"Coherence\": coherence, \"Samples\": samples}\n",
    "\n",
    "    return results\n",
    "\n",
    "# Exemple de vocabulaire et logits simulés\n",
    "vocab = [\"pizza\", \"burger\", \"pasta\", \"salad\", \"sushi\"]\n",
    "logits = np.array([2.0, 1.5, 0.5, 0.2, -0.1])\n",
    "\n",
    "# Paramètres\n",
    "num_samples = 10\n",
    "p = 0.9\n",
    "k = 3\n",
    "temperature = 1.0\n",
    "\n",
    "# Évaluation\n",
    "evaluation_results = evaluate_decoding_methods(vocab, logits, num_samples, p, k, temperature)\n",
    "\n",
    "# Affichage des résultats\n",
    "for method, metrics in evaluation_results.items():\n",
    "    print(f\"Method: {method}\")\n",
    "    print(f\"Diversity: {metrics['Diversity']}\")\n",
    "    print(f\"Coherence: {metrics['Coherence']}\")\n",
    "    print(f\"Samples: {metrics['Samples']}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
