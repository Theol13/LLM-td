{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba450fb1-8a26-4894-ab7a-5d7bfefe90ce",
   "metadata": {},
   "source": [
    "# Chapter 6 - Exercises\n",
    "\n",
    "> Author : Badr TAJINI - Large Language model (LLMs) - ESIEE 2024-2025\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea8be3-30a1-4623-a6d7-b095c6c1092e",
   "metadata": {},
   "source": [
    "## Exercise 6.1: Increasing the context length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1b5925",
   "metadata": {},
   "source": [
    "**Padding Input Sequences in Neural Language Models**\n",
    "\n",
    "**Key Research Question: How does padding inputs to the maximum `token` length affect model predictive performance?**\n",
    "\n",
    "*Methodological Approach:*\n",
    "- Implement systematic `token` padding \n",
    "- Analyze padding's impact on model performance\n",
    "- Explore input representation interactions\n",
    "\n",
    "*Critical Parameters:*\n",
    "- Input `padding` strategy\n",
    "- Maximum `token` length\n",
    "- Predictive performance metrics\n",
    "\n",
    "*Recommended Investigation:*\n",
    "1. Implement maximum-length input `padding`\n",
    "2. Measure performance variations\n",
    "3. Compare padded versus non-padded inputs\n",
    "4. Assess computational implications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc1dc781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration du modèle : {'vocab_size': 50257, 'context_length': 2048, 'drop_rate': 0.0, 'qkv_bias': True, 'emb_dim': 768, 'n_layers': 12, 'n_heads': 12}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from previous_labs import GPTModel\n",
    "\n",
    "# Configuration mise à jour\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,    \n",
    "    \"context_length\": 2048 ,  \n",
    "    \"drop_rate\": 0.0,         \n",
    "    \"qkv_bias\": True          \n",
    "}\n",
    "\n",
    "# Charger les configurations du modèle existant\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# Exemple avec GPT-2 Small\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "# Instancier le modèle avec la nouvelle configuration\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "\n",
    "# Afficher les paramètres pour validation\n",
    "print(f\"Configuration du modèle : {BASE_CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a780455-f52a-48d1-ab82-6afd40bcad8b",
   "metadata": {},
   "source": [
    "## Exercise 6.2: Finetuning the whole model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e23d900",
   "metadata": {},
   "source": [
    "**Model-Wide Fine-Tuning Performance Assessment**\n",
    "\n",
    "**Key Research Question: What is the impact of `fine-tuning` the entire transformer model versus a single final block on predictive performance?**\n",
    "\n",
    "\n",
    "*Methodological Approach:*\n",
    "- Implement comprehensive model `fine-tuning`\n",
    "- Compare performance against single block tuning\n",
    "- Assess computational and representational changes\n",
    "\n",
    "*Critical Parameters:*\n",
    "- Full model `fine-tuning` strategy\n",
    "- Performance evaluation metrics\n",
    "- Comparative analysis methodology\n",
    "\n",
    "*Recommended Investigation:*\n",
    "1. `Fine-tune` entire transformer model\n",
    "2. Measure predictive performance metrics\n",
    "3. Compare with previous single-block tuning results\n",
    "4. Analyze performance variation mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72b8416f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration du modèle pour le fine-tuning complet : {'vocab_size': 50257, 'context_length': 2048, 'drop_rate': 0.0, 'qkv_bias': True, 'emb_dim': 768, 'n_layers': 12, 'n_heads': 12}\n"
     ]
    }
   ],
   "source": [
    "# Débloquer tous les paramètres pour le fine-tuning complet\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Initialiser l'optimiseur pour le fine-tuning complet\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "# Afficher les paramètres pour validation\n",
    "print(f\"Configuration du modèle pour le fine-tuning complet : {BASE_CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2269bce3-f2b5-4a76-a692-5977c75a57b6",
   "metadata": {},
   "source": [
    "## Exercise 6.3: Finetuning the first versus last token "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ddba60",
   "metadata": {},
   "source": [
    "**First Token Fine-Tuning: Predictive Performance Analysis**\n",
    "\n",
    "**Key Research Question: How do predictive performance characteristics change when fine-tuning the first output `token` compared to the last output `token`?**\n",
    "\n",
    "*Methodological Approach:*\n",
    "- Fine-tune first output `token`\n",
    "- Compare performance against last `token` fine-tuning\n",
    "- Assess representational learning variations\n",
    "\n",
    "*Critical Parameters:*\n",
    "- Initial `token` fine-tuning strategy\n",
    "- Performance evaluation metrics\n",
    "- Comparative analysis methodology\n",
    "\n",
    "*Recommended Investigation:*\n",
    "1. Implement first `token` fine-tuning\n",
    "2. Measure predictive performance\n",
    "3. Compare with last `token` fine-tuning results\n",
    "4. Analyze performance variation mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19384e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration du modèle pour le fine-tuning des premiers et derniers tokens : {'vocab_size': 50257, 'context_length': 2048, 'drop_rate': 0.0, 'qkv_bias': True, 'emb_dim': 768, 'n_layers': 12, 'n_heads': 12}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from previous_labs import GPTModel\n",
    "\n",
    "# Configuration mise à jour\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,      \n",
    "    \"context_length\": 2048,  \n",
    "    \"drop_rate\": 0.0,         \n",
    "    \"qkv_bias\": True          \n",
    "}\n",
    "\n",
    "# Charger les configurations du modèle existant\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# Exemple avec GPT-2 Small\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "# Instancier le modèle avec la nouvelle configuration\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "\n",
    "# Débloquer uniquement les premiers et derniers blocs pour le fine-tuning\n",
    "for i, block in enumerate(model.trf_blocks):\n",
    "    if i == 0 or i == len(model.trf_blocks) - 1:\n",
    "        for param in block.parameters():\n",
    "            param.requires_grad = True\n",
    "    else:\n",
    "        for param in block.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Initialiser l'optimiseur pour le fine-tuning sélectif\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5, weight_decay=0.1\n",
    ")\n",
    "\n",
    "# Afficher les paramètres pour validation\n",
    "print(f\"Configuration du modèle pour le fine-tuning des premiers et derniers tokens : {BASE_CONFIG}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
