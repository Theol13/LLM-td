{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c503e5ef-6bb4-45c3-ac49-0e016cedd8d0",
   "metadata": {},
   "source": [
    "# Lab 1b (bonus for deep understanding) - Chapter 2 \n",
    "> Author : Badr TAJINI - Large Language model (LLMs) - ESIEE 2024-2025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff73d6c6",
   "metadata": {},
   "source": [
    "#### 1. Package Installation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9e554f-58e3-4787-832d-d149add1b857",
   "metadata": {},
   "source": [
    "- Install the additional package requirements for this bonus notebook by uncommenting and running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d70bae22-b540-4a13-ab01-e748cb9d55c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\theo.labat\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements-extra.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\theo.labat\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from -r requirements-extra.txt (line 2)) (4.66.2)\n",
      "Collecting transformers>=4.33.2 (from -r requirements-extra.txt (line 3))\n",
      "  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\theo.labat\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->-r requirements-extra.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\theo.labat\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->-r requirements-extra.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\theo.labat\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->-r requirements-extra.txt (line 1)) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\theo.labat\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->-r requirements-extra.txt (line 1)) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\theo.labat\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->-r requirements-extra.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\theo.labat\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers>=4.33.2->-r requirements-extra.txt (line 3)) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers>=4.33.2->-r requirements-extra.txt (line 3))\n",
      "  Downloading huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\theo.labat\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers>=4.33.2->-r requirements-extra.txt (line 3)) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\theo.labat\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers>=4.33.2->-r requirements-extra.txt (line 3)) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\theo.labat\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers>=4.33.2->-r requirements-extra.txt (line 3)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\theo.labat\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers>=4.33.2->-r requirements-extra.txt (line 3)) (2024.11.6)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers>=4.33.2->-r requirements-extra.txt (line 3))\n",
      "  Downloading tokenizers-0.20.3-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers>=4.33.2->-r requirements-extra.txt (line 3))\n",
      "  Downloading safetensors-0.4.5-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\theo.labat\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.33.2->-r requirements-extra.txt (line 3)) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\theo.labat\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.33.2->-r requirements-extra.txt (line 3)) (4.8.0)\n",
      "Downloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   --------------------------------- ------ 8.4/10.0 MB 47.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.0/10.0 MB 36.7 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.26.3-py3-none-any.whl (447 kB)\n",
      "Downloading safetensors-0.4.5-cp311-none-win_amd64.whl (285 kB)\n",
      "Downloading tokenizers-0.20.3-cp311-none-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 33.6 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.26.3 safetensors-0.4.5 tokenizers-0.20.3 transformers-4.46.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements-extra.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8ddfe7",
   "metadata": {},
   "source": [
    "This setup block ensures all necessary dependencies are available for the comparative analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737c59bb-5922-46fc-a787-1369d70925b4",
   "metadata": {},
   "source": [
    "# Comparing Various Byte Pair Encoding (BPE) Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9adc3bf-353c-411e-a471-0e92786e7103",
   "metadata": {},
   "source": [
    "#### 2. Tiktoken Implementation:\n",
    "<br>\n",
    "&nbsp;\n",
    "\n",
    "#### Using BPE from `tiktoken`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c490fca-a48a-47fa-a299-322d1a08ad17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.8.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0952667c-ce84-4f21-87db-59f52b44cec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tik_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = \"Hello, world. Is this-- a test?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c5032d",
   "metadata": {},
   "source": [
    "This section introduces OpenAI's tiktoken library (version 0.5.1), which represents a modern, optimized implementation of BPE tokenization. The code initializes the GPT-2 encoding scheme and defines a test string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04370eaf",
   "metadata": {},
   "source": [
    "#### 3. Tiktoken Encoding/Decoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b039c350-18ad-48fb-8e6a-085702dfc330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 995, 13, 1148, 428, 438, 257, 1332, 30]\n"
     ]
    }
   ],
   "source": [
    "integers = tik_tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b152ba4-04d3-41cc-849f-adedcfb8cabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world. Is this-- a test?\n"
     ]
    }
   ],
   "source": [
    "strings = tik_tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf148a1a-316b-43ec-b7ba-1b6d409ce837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    }
   ],
   "source": [
    "print(tik_tokenizer.n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643cc724",
   "metadata": {},
   "source": [
    "This demonstrates the bidirectional conversion between text and token IDs. The output reveals that GPT-2's vocabulary contains 50,257 tokens, a carefully chosen size balancing coverage and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0b5d4f-2af9-40de-828c-063c4243e771",
   "metadata": {},
   "source": [
    "#### 4. Original GPT-2 BPE Implementation:\n",
    "<br>\n",
    "&nbsp;\n",
    "\n",
    "####  Using the original BPE implementation used in GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0903108c-65cb-4ae1-967a-2155e25349c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpe_openai_gpt2 import get_encoder, download_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35dd8d7c-8c12-4b68-941a-0fd05882dd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching encoder.json: 1.04Mit [00:01, 607kit/s]                                                    \n",
      "Fetching vocab.bpe: 457kit [00:01, 371kit/s]                                                        \n"
     ]
    }
   ],
   "source": [
    "download_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1888a7a9-9c40-4fe0-99b4-ebd20aa1ffd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_tokenizer = get_encoder(model_name=\"gpt2_model\", models_dir=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2740510c-a78a-4fba-ae18-2b156ba2dfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 995, 13, 1148, 428, 438, 257, 1332, 30]\n"
     ]
    }
   ],
   "source": [
    "integers = orig_tokenizer.encode(text)\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "434d115e-990d-42ad-88dd-31323a96e10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world. Is this-- a test?\n"
     ]
    }
   ],
   "source": [
    "strings = orig_tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cbc7f8",
   "metadata": {},
   "source": [
    "This section implements the original GPT-2 tokenization scheme, providing a baseline for comparison. The vocabulary files are downloaded to ensure consistency with the original implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f63e8c6-707c-4d66-bcf8-dd790647cc86",
   "metadata": {},
   "source": [
    "#### 5. Hugging Face Transformers Implementation:\n",
    "<br>\n",
    "&nbsp;\n",
    "\n",
    "#### Using the BPE via Hugging Face transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9077bf4-f91f-42ad-ab76-f3d89128510e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.46.3'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9839137-b8ea-4a2c-85fc-9a63064cf8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b19cee63f7f47d78c301b0aa7eb4ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\theo.labat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\theo.labat\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186a221b917644529b13c3c3f5c44cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d26d3f7e5047139f54dd9028a8ca3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c6212e9fb74d2b820f4f831b9514d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b35135ee2f2c4b218bbbae38388a70d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "hf_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "222cbd69-6a3d-4868-9c1f-421ffc9d5fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15496, 11, 995, 13, 1148, 428, 438, 257, 1332, 30]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer(strings)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4507ebce",
   "metadata": {},
   "source": [
    "This introduces the Hugging Face implementation, which has become a de facto standard in the research community due to its accessibility and integration with the broader transformers ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907a1ade-3401-4f2e-9017-7f58a60cbd98",
   "metadata": {},
   "source": [
    "#### 6. Performance Benchmark:\n",
    "<br>\n",
    "&nbsp;\n",
    "\n",
    "#### A quick performance benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a61bb445-b151-4a2f-8180-d4004c503754",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\theo.labat\\\\Documents\\\\LLM\\lab2\\\\1_main_code\\\\the-verdict.txt', 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57f7c0a3-c1fd-4313-af34-68e78eb33653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.6 ms ± 2.97 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit orig_tokenizer.encode(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "036dd628-3591-46c9-a5ce-b20b105a8062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.99 ms ± 447 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit tik_tokenizer.encode(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9c85b58-bfbc-465e-9a7e-477e53d55c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5145 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.3 ms ± 2.31 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit hf_tokenizer(raw_text)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7117107f-22a6-46b4-a442-712d50b3ac7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.3 ms ± 4.28 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit hf_tokenizer(raw_text, max_length=5145, truncation=True)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f676962c",
   "metadata": {},
   "source": [
    "The benchmark reveals significant performance differences:\n",
    "\n",
    "- Original implementation: ~4.29ms\n",
    "- Tiktoken: ~1.4ms\n",
    "- Hugging Face: ~8.46ms\n",
    "\n",
    "This performance comparison highlights tiktoken's optimization, achieving roughly 3x faster processing than the original implementation and 6x faster than Hugging Face's version. These differences become particularly significant when processing large volumes of text for training or inference.\n",
    "The notebook effectively demonstrates the evolution of BPE implementation in the context of GPT-2, from the original version to more recent optimized implementations. The consistent tokenization results across implementations validate their compatibility, while the performance metrics illuminate the practical implications of implementation choices in production environments.\n",
    "This analysis provides valuable insights selecting tokenization implementations based on specific requirements for speed, compatibility, and ecosystem integration.\n",
    "\n",
    "END."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
